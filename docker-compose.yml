networks:
  elk-network:
    driver: bridge

services:
  # 1. ModSecurity Proxy (Flask + ModSecurity)
  modsec-proxy:
    build: ./modsec-proxy
    container_name: modsec-proxy
    ports:
      - "8080:80" # 외부에서 80 포트로 접근
    volumes:
      # modsec-logs 디렉토리를 ModSecurity 로그가 기록되는 경로에 볼륨 마운트
      - ./modsec_logs:/var/log/apache2/
    networks:
      - elk-network

  # 2. Filebeat
  filebeat:
    build: ./elk-stack/filebeat
    container_name: filebeat
    volumes:
      - ./modsec_logs:/var/log/apache2/:ro
      - ./elk-stack/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
    entrypoint: "filebeat -e --strict.perms=false -c /usr/share/filebeat/filebeat.yml"
    depends_on:
      - elasticsearch
      - modsec-proxy
    restart: on-failure
    networks:
      - elk-network

  # 3. Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false # 개발/테스트용. 실제 운영에서는 보안 활성화 필수!
      - ES_JAVA_OPTS=-Xms512m -Xmx512m # 메모리 설정 (필요에 따라 조정)
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      # Elasticsearch 데이터 지속성을 위한 볼륨 마운트 (데이터 유실 방지)
      - es_data:/usr/share/elasticsearch/data
    networks:
      - elk-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # 4. Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.4 # Elasticsearch와 동일 버전
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 # Elasticsearch 서비스 이름 사용
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - elk-network

  # 5. Ollama
  ollama:
    image: ollama/ollama:latest # 최신 Ollama 이미지 사용
    container_name: ollama
    ports:
      - "11434:11434" # Ollama API 포트
    volumes:
      # Ollama 모델 저장소 (모델 다운로드 후 컨테이너 재시작해도 유지)
      - ollama_models:/root/.ollama
    networks:
      - elk-network
    # 명령어로 특정 모델을 자동으로 pull하려면 아래 주석 해제 (단, 첫 실행 시 시간이 오래 걸릴 수 있음)
    # command: ["/bin/bash", "-c", "ollama pull llama3 && ollama serve"]


  # 6. Flask API (AI Log Analyzer)
  ai-log-analyzer:
    build: ./ai-log-analyzer # AI Log Analyzer Dockerfile 경로
    container_name: ai-log-analyzer
    environment:
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - OLLAMA_API_URL=http://ollama:11434/api/generate
      - LLM_MODEL_NAME=llama3 # 사용할 LLM 모델 이름 (Ollama에 미리 pull 되어 있어야 함)
    ports:
      - "5000:5000" # 외부에서 5000 포트로 접근
    depends_on:
      - elasticsearch
      - ollama
    networks:
      - elk-network

volumes:
  es_data: # Elasticsearch 데이터 볼륨
  ollama_models: # Ollama 모델 볼륨